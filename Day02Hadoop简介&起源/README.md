### 什么是大数据?
+ 定义：大数据是指无法再一定时间内用常规的软件工具对齐内容进行抓取、管理和处理的数据集合。大数据技术，是指
从各种各样的类型的数据中，快速获得由价值信息的能力。适用于大数据的技术，包括大规模并行处理(MPP)数据库，数据
挖掘，分布式系统，分布式数据库，云计算平台，互联网，和可扩展的存储系统
+ 市场需求：传统的数据处理技术无法胜任，需要催生新的技术。一套用来处理海量数据的软件工具应运而生，这就是大数据。
+ 技术层面：数据处理技术
+ 处理方式：大数据并行化处理数据
+ 处理海量技术的核心技术：分布式
> + 海量的数据存储:分布式
> + 海量的数据计算：分布式
+ ★★★★★ 存储和计算成熟的框架
> + 数据存储：
> > + ★HDFS：分布式文件系统(Hadoop的存储框架)，后续的HIVE HBASE底层存储数据都是HDFS
> > + ★Hbase：分布式数据库系统(HDFS的二次封装)
> > + ★KAFKA：分布式消息缓存系统
> + 计算框架:
> > + ★MapReduce：离线计算框架(Hadoop的计算框架，主要学习核心事项，后续的Flink Spark都会用到该思想)
> > + ★Spark：离线批处理、在线流式计算、机器学习等计算框架
> > + Storm：实时流式计算框架
> > + ★Flink：实时流式计算框架
> + 辅助工具：
> > + ★HIVE：数据仓库工具，可以直接使用SQL
> > + ★Flume：数据采集工具
> > + ★Sqoop：数据迁移工具(传统数据库到分布式，分布式到传统数据库)
> > + ★Zookeeper：分布式协调工具
> > + ★Azkaban：分布式任务调度工具
+ 大数据运用场景
> + 公司运营情况
> + 电商推荐系统：如淘宝、京东、苏宁，大量基于算法模型的运行，的出来各种推荐结论
> + 广告推荐系统：基于海量互联网用户的各类数据
+ 什么是Hadoop
> + 官网：https://hadoop.apache.org/
> + 简介：Apache™Hadoop®项目开发了用于`可靠，可扩展的分布式计算`的开源软件。Apache Hadoop软件库是一个框架，
允许使用简单的`编程模型跨计算机集群分布式处理大型数据集`。它旨在从单个服务器`扩展到数千台计算机`，每台计算机都提供本地计算和存储。库本身不是依靠硬件来提供高可用性，而是设计用于检测和处理应用程序层的故障，从而在计算机集群之上提供高可用性服务，每个计算机都可能容易出现故障。
> + 特点：用户可以在不了解分布式底层细节而开发分布式程序
> + 核心组件：
> > + 分布式文件系统:HDFS 实现存储在多台服务器上
> > + 分布式运算编程框架:MapReduce 实现多台机器的分布式并计算
> > + 分布式资源调度平台：Yarn 调度大量的MapReduce程序，并且合理的分配运算数据
+ HDFS的运行机制
> - 用户文件会被切块后存储在多台DataNode服务器当中，并且每个文件在整个集群中存放多个副本，可以提高数据的安全性
> > - 对用户提供统一的目录，存储时会把文件切分为若干个块存储在不同DataNode中
> > - 用户可以设置多个数据副本，以增强数据的安全性
> > - 用户存储的信息放在NameNode中
> - NameNode：整个文件系统的管理节点，接收用户的请求，保存这文件/目录的元数据信息和每个文件对应的block映射表。在Linux
中，保存这三个重要的文件
> > - fsimage：元数据镜像文件，存储一段时间内NameNode的元数据信息
> > - edits：保存操作日志文件
> > - fstime：保存最近一次checkpoint的时间
